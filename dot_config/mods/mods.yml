# Default model (gpt-3.5-turbo, gpt-4, ggml-gpt4all-j...).
default-model: meta-llama/llama-4-scout-17b-16e-instruct
# Text to append when using the -f flag.
format-text:
  markdown: "Format the response as markdown without enclosing backticks."
  json: "Format the response as json without enclosing backticks."
  csv: "Format the response as csv without enclosing backticks."
# List of predefined system messages that can be used as roles.
roles:
  "default": []
  "issue-docsets":
    - your job is to determine the which docs are needed for a Github issue
    - you do not explain anything
    - you are given the title, scope, and list of docs for a Github repo in that order
    - you determine the docs by analyzing the keywords in the docs for each url
    - you return a markdown list of the docs that are needed with links to the docs
    - you return no more than 4 docs per issue
  "issue-synopsis":
    - your job is to determine the synopsis of a Github issue
    - you do not explain anything
    - you are given the title scope and list of docs for a Github repo in that order
    - you return 2-3 sentences that describe the issue
  "branch-namer":
    - you are a PR titler
    - you do not explain anything
    - you read the provided issue and body then create a title for the branch
    - you do not provide any explanation whatsoever, ONLY the title
    - titles must be no more than 3 words (example = feature/add-new-feature)
    - ONLY return the title without feature prefix

# Ask for the response to be formatted as markdown unless otherwise set.
format: true
# System role to use.
role: "default"
# Render output as raw text when connected to a TTY.
raw: false
# Quiet mode (hide the spinner while loading and stderr messages for success).
quiet: true
# Temperature (randomness) of results, from 0.0 to 2.0.
temp: 1.0
# TopP, an alternative to temperature that narrows response, from 0.0 to 1.0.
topp: 1.0
# Turn off the client-side limit on the size of the input into the model.
no-limit: false
# Wrap formatted output at specific width (default is 80)
word-wrap: 80
# Include the prompt from the arguments in the response.
include-prompt-args: false
# Include the prompt from the arguments and stdin, truncate stdin to specified number of lines.
include-prompt: 0
# Maximum number of times to retry API calls.
max-retries: 5
# Your desired level of fanciness.
fanciness: 10
# Text to show while generating.
status-text: Generating
# Theme to use in the forms. Valid units are: 'charm', 'catppuccin', 'dracula', and 'base16'
theme: base16
# Default character limit on input to model.
max-input-chars: 12250
# Maximum number of tokens in response.
# max-tokens: 100
# Aliases and endpoints for OpenAI compatible REST API.
apis:
  openai:
    base-url: https://api.openai.com/v1
    api-key:
    api-key-env: OPENAI_API_KEY
    models: # https://platform.openai.com/docs/models
      gpt-4o-mini:
        aliases: ["4o-mini"]
        max-input-chars: 392000
        fallback: gpt-4o
      gpt-4o:
        aliases: ["4o"]
        max-input-chars: 392000
        fallback: gpt-4
      gpt-4:
        aliases: ["4"]
        max-input-chars: 24500
        fallback: gpt-3.5-turbo
      gpt-4-1106-preview:
        aliases: ["128k"]
        max-input-chars: 392000
        fallback: gpt-4
      gpt-4-32k:
        aliases: ["32k"]
        max-input-chars: 98000
        fallback: gpt-4
      gpt-3.5-turbo:
        aliases: ["35t"]
        max-input-chars: 12250
        fallback: gpt-3.5
      gpt-3.5-turbo-1106:
        aliases: ["35t-1106"]
        max-input-chars: 12250
        fallback: gpt-3.5-turbo
      gpt-3.5-turbo-16k:
        aliases: ["35t16k"]
        max-input-chars: 44500
        fallback: gpt-3.5
      gpt-3.5:
        aliases: ["35"]
        max-input-chars: 12250
        fallback:
  anthropic:
    base-url: https://api.anthropic.com/v1
    api-key: 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
    models: # https://docs.anthropic.com/en/docs/about-claude/models
      claude-3-5-sonnet-20240620:
        aliases: ["claude3.5-sonnet", "claude-3-5-sonnet", "sonnet-3.5"]
        max-input-chars: 680000
      claude-3-opus-20240229:
        aliases: ["claude3-opus", "opus"]
        max-input-chars: 680000
  cohere:
    base-url: https://api.cohere.com/v1
    models:
      command-r-plus:
        max-input-chars: 128000
      command-r:
        max-input-chars: 128000
  ollama:
    base-url: http://localhost:11434/api
    models: # https://ollama.com/library
      "llama3:70b":
        aliases: ["llama3"]
        max-input-chars: 650000
  perplexity:
    base-url: https://api.perplexity.ai
    api-key: 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
    models: # https://docs.perplexity.ai/docs/model-cards
      llama-3-sonar-small-32k-chat:
        aliases: ["llama3-ss"]
        max-input-chars: 32768
      llama-3-sonar-small-32k-online:
        aliases: ["llam3-sso"]
        max-input-chars: 28000
      llama-3-sonar-large-32k-chat:
        aliases: ["llam3-sl"]
        max-input-chars: 32768
      llama-3-sonar-large-32k-online:
        aliases: ["llam3-slo"]
        max-input-chars: 28000
      llama-3-8b-instruct:
        aliases: ["llam3-8bi"]
        max-input-chars: 8192
      llama-3-70b-instruct:
        aliases: ["llam3-70bi"]
        max-input-chars: 8192
  groq:
    base-url: https://api.groq.com/openai/v1
    api-key: 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
    models: # https://console.groq.com/docs/models
      meta-llama/llama-4-scout-17b-16e-instruct:
        aliases: ["llama4-scout"]
        max-input-chars: 128000
      gemma-7b-it:
        aliases: ["gemma"]
        max-input-chars: 24500
      gemma2-9b-it:
        aliases: ["gemma2"]
        max-input-chars: 24500
      llama3-70b-8192:
        aliases: ["llama3", "llama3-70b"]
        max-input-chars: 24500
        fallback: llama3-8b-8192
      llama3.3-70b-versatile:
        aliases: ["llama3.3"]
        max-input-chars: 128000
      mixtral-8x7b-32768:
        aliases: ["mixtral"]
        max-input-chars: 98000
